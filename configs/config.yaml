name: baseline+smart_init
model: facebook/bart-large
device: cuda:0
pt_folder_name: mask_00110_6_0-7-amr3.0

# Add clause token visibility mask into weights attention mask of Encoder's SelfAttention Layer
clause_token_visibility: True
clause_token_inf_mask: False
attn_rate_same_clause: 2.0
attn_rate_adjacent_clause: 0.5
attn_rate_nonadjacent_clause: 0.00001
attn_rate_keyword_token: 0.00001
attn_rate_reen_token: 1.0
clause_attn_head_num: 6
clause_attn_layer_id: 0 1 2 3 4 5 6 7

# <--------------
# Linearizations
# Comment DFS and uncomment the relevant block if you want to use a different linearization scheme

# DFS
penman_linearization: True
use_pointer_tokens: True
raw_graph: False

# BF
# penman_linearization: False
# use_pointer_tokens: True
# raw_graph: False

# PENMAN
# penman_linearization: True
# use_pointer_tokens: False
# raw_graph: False

# BART baseline
# penman_linearization: True
# use_pointer_tokens: False
# raw_graph: True

remove_wiki: False
dereify: False
collapse_name_ops: False

# Hparams
batch_size: 500
beam_size: 1
dropout: 0.25
attention_dropout: 0.0
smart_init: True
accum_steps: 10
warmup_steps: 1
training_steps: 250000
weight_decay: 0.004
grad_norm: 2.5
scheduler: constant
learning_rate: 0.00005
max_epochs: 30
save_checkpoints: True
log_wandb: False
warm_start: True
use_recategorization: False
best_loss: False
remove_longer_than: 1024

# <------------------

train: /mnt/sda1_hd/fanyunlong/amr_parsing/data/AMR/amr_3.0_clause/train.txt
dev: /mnt/sda1_hd/fanyunlong/amr_parsing/data/AMR/amr_3.0_clause/dev.txt
test: /mnt/sda1_hd/fanyunlong/amr_parsing/data/AMR/amr_3.0_clause/test.txt

train_features: /mnt/sda1_hd/fanyunlong/amr_parsing/data/AMR/amr_3.0_clause/train.txt.features
dev_features: /mnt/sda1_hd/fanyunlong/amr_parsing/data/AMR/amr_3.0_clause/dev.txt.features
test_features: /mnt/sda1_hd/fanyunlong/amr_parsing/data/AMR/amr_3.0_clause/test.txt.features
